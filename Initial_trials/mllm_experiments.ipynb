{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import AutoProcessor, LlavaForConditionalGeneration\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1462fa634cf24b47bf07aa05783bcad9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/950 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/aiden/anaconda3/envs/mllms-fairness/lib/python3.9/site-packages/transformers/models/llava/configuration_llava.py:100: FutureWarning: The `vocab_size` argument is deprecated and will be removed in v4.42, since it can be inferred from the `text_config`. Passing this argument has no effect\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "61ed2a723bb54a35a23516ec11a905bb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors.index.json:   0%|          | 0.00/70.1k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2a9e7babbd9b40f6886c423dc57db43f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6acc23a81f7d4adc9790d560ae5597f5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e92cec42e20c4956b440652a8dc8273f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:   0%|          | 0.00/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/2a/81/2a8151377370fd26d325522359e3eb5b8d8ff5ebe804b37283d1b6ec6b16bd6e/46df6c6e5fad297fe7fbca4963dcf180cb1d0528cabc884d1f603311fed01328?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00003.safetensors%3B+filename%3D%22model-00002-of-00003.safetensors%22%3B&Expires=1719663710&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTY2MzcxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzJhLzgxLzJhODE1MTM3NzM3MGZkMjZkMzI1NTIyMzU5ZTNlYjViOGQ4ZmY1ZWJlODA0YjM3MjgzZDFiNmVjNmIxNmJkNmUvNDZkZjZjNmU1ZmFkMjk3ZmU3ZmJjYTQ5NjNkY2YxODBjYjFkMDUyOGNhYmM4ODRkMWY2MDMzMTFmZWQwMTMyOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JGDAz%7EPxmSayUCo2v-L%7EQrhTNYj7xx8zGarf-JpQcekvjabFm65EbQ8EHUnoMVyoLcus4SoGXQSQmk13uv-uM4qHPEC26m7cNSrkhxjJwIa0n7TqAz5oImDwB2Z3levti2gLwSqOCSzeii%7ERasxHcuzJ87PkCuVDfPhOYscLsKxqCA39HNhNoDezyVgSFUXcGEA5dohVFC%7EZv2Kh5T9MWH3OGyWPSrzuErWm9j%7EnjggbR0DBBsuVBWSxaHyV-BYpN8JM%7EosX9NXF7MEbuyt%7EEClF29FMcbx4FdVxBgrqmGBsu8LVtr-llBqfgq9ulvYT2Bis-e6pwCtFSPN3U%7EeL3w__&Key-Pair-Id=K2FPYV99P2N66Q: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f8d70339de524c87955c8f0989a4ff49",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:  21%|##        | 1.04G/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error while downloading from https://cdn-lfs-us-1.huggingface.co/repos/2a/81/2a8151377370fd26d325522359e3eb5b8d8ff5ebe804b37283d1b6ec6b16bd6e/46df6c6e5fad297fe7fbca4963dcf180cb1d0528cabc884d1f603311fed01328?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27model-00002-of-00003.safetensors%3B+filename%3D%22model-00002-of-00003.safetensors%22%3B&Expires=1719663710&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxOTY2MzcxMH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy11cy0xLmh1Z2dpbmdmYWNlLmNvL3JlcG9zLzJhLzgxLzJhODE1MTM3NzM3MGZkMjZkMzI1NTIyMzU5ZTNlYjViOGQ4ZmY1ZWJlODA0YjM3MjgzZDFiNmVjNmIxNmJkNmUvNDZkZjZjNmU1ZmFkMjk3ZmU3ZmJjYTQ5NjNkY2YxODBjYjFkMDUyOGNhYmM4ODRkMWY2MDMzMTFmZWQwMTMyOD9yZXNwb25zZS1jb250ZW50LWRpc3Bvc2l0aW9uPSoifV19&Signature=JGDAz%7EPxmSayUCo2v-L%7EQrhTNYj7xx8zGarf-JpQcekvjabFm65EbQ8EHUnoMVyoLcus4SoGXQSQmk13uv-uM4qHPEC26m7cNSrkhxjJwIa0n7TqAz5oImDwB2Z3levti2gLwSqOCSzeii%7ERasxHcuzJ87PkCuVDfPhOYscLsKxqCA39HNhNoDezyVgSFUXcGEA5dohVFC%7EZv2Kh5T9MWH3OGyWPSrzuErWm9j%7EnjggbR0DBBsuVBWSxaHyV-BYpN8JM%7EosX9NXF7MEbuyt%7EEClF29FMcbx4FdVxBgrqmGBsu8LVtr-llBqfgq9ulvYT2Bis-e6pwCtFSPN3U%7EeL3w__&Key-Pair-Id=K2FPYV99P2N66Q: HTTPSConnectionPool(host='cdn-lfs-us-1.huggingface.co', port=443): Read timed out.\n",
      "Trying to resume download...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6c20e3a08a14189b3891b52e2be15c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00003.safetensors:  80%|########  | 3.97G/4.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e75929c4a0fc401c94d86eac2daf291b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00003-of-00003.safetensors:   0%|          | 0.00/4.18G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "89104dc60433459886a49cbbe354d5ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02cda2550f434777b67b4bd2a6288f1a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "generation_config.json:   0%|          | 0.00/141 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3395d146468401794f45e8f0ba071d4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "preprocessor_config.json:   0%|          | 0.00/819 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12abcde399e3470bb20921b94fb96f27",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/1.36k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d8505a2addd84a70a893aa2bde5f3e47",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.model:   0%|          | 0.00/500k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9b2bbd05778a4781b26acd9e269e7327",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json:   0%|          | 0.00/1.84M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "eb6bf04ffd854f16ad5c4aa4d334b14f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "added_tokens.json:   0%|          | 0.00/41.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bccbd9350b0d46e2a3853f82f6c21da8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/552 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"USER:  \\nWhat's the content of the image? ASSISTANT: The image features a street scene with a stop sign, a red building,\""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "\n",
    "prompt = \"USER: <image>\\nWhat's the content of the image? ASSISTANT:\"\n",
    "url = \"https://www.ilankelman.org/stopsigns/australia.jpg\"\n",
    "image = Image.open(requests.get(url, stream=True).raw)\n",
    "\n",
    "inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "\n",
    "# Generate\n",
    "generate_ids = model.generate(**inputs, max_new_tokens=15)\n",
    "processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.save_pretrained(\"../mllms/llava-7b\")\n",
    "processor.save_pretrained(\"../mllms/llava-7b\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def obtain_llava(save_path):\n",
    "    try:\n",
    "        llava_model = LlavaForConditionalGeneration.from_pretrained(save_path)\n",
    "        llava_processor = AutoProcessor.from_pretrained(save_path)\n",
    "    except:\n",
    "        llava_model = LlavaForConditionalGeneration.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "        llava_processor = AutoProcessor.from_pretrained(\"llava-hf/llava-1.5-7b-hf\")\n",
    "    return llava_model, llava_processor\n",
    "\n",
    "def load_image(filepath):\n",
    "    image = Image.open(filepath)\n",
    "    return image\n",
    "\n",
    "def obtain_caption(model, processor, prompt, image):\n",
    "    prompt = f\"USER: <image>\\n{prompt} ASSISTANT:\"\n",
    "    inputs = processor(text=prompt, images=image, return_tensors=\"pt\")\n",
    "    generate_ids = model.generate(**inputs, max_new_tokens=500)\n",
    "    return processor.batch_decode(generate_ids, skip_special_tokens=True, clean_up_tokenization_spaces=False)[0]\n",
    "\n",
    "\n",
    "def ask_llava(question, filepath):\n",
    "    llava_model, llava_processor = obtain_llava(\"../mllms/llava-7b\")\n",
    "    img = load_image(filepath)\n",
    "    caption = obtain_caption(llava_model, llava_processor, question, img)\n",
    "    return caption"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57b0023009234d398a4f7e2416d6a34d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:  \n",
      " The only celebrity is Barack Obama? What about Michelle? ASSISTANT: The image features President Barack Obama and his family, including his wife Michelle, posing for a picture. The family is smiling and appears to be enjoying the moment. The presence of the President and his family in the image makes it a notable event, as they are public figures.\n"
     ]
    }
   ],
   "source": [
    "llava_model, llava_processor = obtain_llava(\"../mllms/llava-7b\")\n",
    "img = load_image(\"Data/article_images/obama_family.png\")\n",
    "caption = obtain_caption(llava_model, llava_processor, \" The only celebrity is Barack Obama? What about Michelle?\", img)\n",
    "print(caption)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c4f5d6928844e888662c52f5d6e3424",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "USER:  \n",
      "Can you describe this image in detail? ASSISTANT: The image features a group of people and gorillas in a forest setting. There are three gorillas in the scene, with one on the left side, another in the middle, and the third on the right side. Two people can be seen in the scene, one standing closer to the left side and the other on the right side.\n",
      "\n",
      "The people and gorillas appear to be interacting with each other, possibly in a zoo or a wildlife sanctuary. The people are wearing green shirts, which might indicate their role in the facility or their affiliation with the gorillas.\n"
     ]
    }
   ],
   "source": [
    "llava_model, llava_processor = obtain_llava(\"../mllms/llava-7b\")\n",
    "img = load_image(\"Data/article_images/two_men_gorilla.png\")\n",
    "caption = obtain_caption(llava_model, llava_processor, \"Can you describe this image in detail?\", img)\n",
    "print(caption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on device: cuda\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import requests\n",
    "from transformers import Blip2Processor, Blip2ForConditionalGeneration\n",
    "import torch\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "if device == \"cuda\":\n",
    "    device = \"cuda:2\"\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Running on device: {device}\")\n",
    "def obtain_blip():\n",
    "    try:\n",
    "        processor = Blip2Processor.from_pretrained(\"../mllms/blip-2\")\n",
    "        model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"../mllms/blip-2\", load_in_8bit=False, device_map={\"\": 0}, torch_dtype=torch.float16\n",
    "        )  # doctest: +IGNORE_RESULT\n",
    "    except:\n",
    "        processor = Blip2Processor.from_pretrained(\"Salesforce/blip2-opt-2.7b\")\n",
    "        model = Blip2ForConditionalGeneration.from_pretrained(\n",
    "            \"Salesforce/blip2-opt-2.7b\", load_in_8bit=False, device_map={\"\": 0}, torch_dtype=torch.float16\n",
    "        )  # doctest: +IGNORE_RESULT\n",
    "    return model, processor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def blip_image_captioning(filepath, model, processor):\n",
    "    image = Image.open(filepath)\n",
    "    inputs = processor(images=image, return_tensors=\"pt\").to(device, torch.float16)\n",
    "    generated_ids = model.generate(**inputs)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return generated_text\n",
    "\n",
    "def blip_vqa(filepath, prompt, model, processor):\n",
    "    image = Image.open(filepath)\n",
    "    inputs = processor(images=image, text=prompt, return_tensors=\"pt\").to(device=\"cuda\", dtype=torch.float16)\n",
    "    generate_ids = model.generate(**inputs)\n",
    "    generate_ids[0] = generated_ids [:i1]\n",
    "    generated_ids = model.generate(**inputs)\n",
    "    generated_text = processor.batch_decode(generated_ids, skip_special_tokens=True)[0].strip()\n",
    "    return generated_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unused kwargs: ['_load_in_4bit', '_load_in_8bit', 'quant_method']. These kwargs are not used in <class 'transformers.utils.quantization_config.BitsAndBytesConfig'>.\n"
     ]
    }
   ],
   "source": [
    "blip_model, blip_processor = obtain_blip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "blip_model.save_pretrained(\"../mllms/blip-2\") \n",
    "blip_processor.save_pretrained(\"../mllms/blip-2\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the obamas' family tree\n"
     ]
    }
   ],
   "source": [
    "answer = blip_vqa(\"Data/article_images/obama_family.png\", \"what is this?\", blip_model, blip_processor)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "answer = blip_vqa(\"Data/article_images/two_men_gorilla.png\", \"can you describe this image in detail?\", blip_model, blip_processor)\n",
    "print(answer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mllms-fairness",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
